<!DOCTYPE html>
<html>
  <head>
    <title>Neutron Quality of Service, new features and future roadmap</title>
    <meta charset="utf-8">
    <link rel="stylesheet" href="style.css" />
  </head>
  <body>
    <textarea id="source">

class: center, middle

# Neutron Quality of Service, new features and future roadmap

---

# Agenda

1. The RBAC functionality in Mitaka
2. QoS in Liberty - reminder
3. Bandwidth limit for Linuxbridge agent
4. DSCP functionality in Neuton
5. The possibilities for the future.

---
class: center

# QoS in Liberty

## Egress bandwidth limit for:

* Openvswitch L2 agent
* SR-IOV L2 agent

???

In Liberty:
* QoS only with one rule type: egress traffic limiting
* implementation only for ovs and SR-IOV L2 agents.

---
class: center

# Egress bandwidth limit rule:

1. Create policy
```
neutron qos-policy-create example-policy
```
2. Create bandwidth limit rule:
```
neutron qos-bandwidth-limit-rule-create \
    --max-kbps 512 \
    --max-burst 1024 \
    example-policy
```
3. Apply policy for port/network
```
neutron port-update 071e54ad-d5f5-402f-933f-dece0db3afde \
    --qos-policy example-policy
```

???

Create bandwith limit (EGRESS) require steps like on slide.

---
name: qos_bw_limit_ovs
class: center

# Apply egress bandwidth limit rule in Openvswitch agent

```
ovs-vsctl set Interface tap1 ingress_policing_rate=512
ovs-vsctl set Interface tap1 ingress_policing_burst=1024
```

![OVS Bridge connection](images/OVS_Bridge_connection.png)


???
In Liberty BW Limit rule could be applied by Openvswitch and SR-IOV agent.
Example: OVS agent makes it like on slide

---

name: qos_linuxbridge_support_1
class: center

# Egress bandwidth limit in Mitaka

## support for linuxbridge l2 agent

![Linuxbridge agent support arrived](images/Agent-arrived.jpg)


???
In Mitaka support for egress bw limit for linuxbridge agent.
From user POV same as for OVS agent (policy, rule, apply rule to port/network)
Implementation details on next slide...

---

name: qos_linuxbridge_details_1
class: center

# Linuxbridge agent details

* Linuxbridge agent use tc (traffic control) Linux tool
* policing on ingress qdisc (queueing discipline)
* Token Bucket Filter used on ingress qdisc

![Egress traffic explanation](images/Egress_traffic.png)

???
- Using basic Linux tool tc to apply qos rules
- qdisc is short for 'queueing discipline' and it is elementary to under-
  standing traffic control. Whenever the kernel needs to send a packet to
  an  interface,  it  is enqueued to the qdisc configured for that inter-
  face. Immediately afterwards, the kernel tries to get as  many  packets
  as  possible  from  the  qdisc,  for giving them to the network adaptor
  driver. (from tc manpage)
- using ingress qdisc because it is treated from bridge POV. So from VM it will
  be egress traffic

---

name: qos_linuxbridge_details_2
class: center

# Example with Linuxbridge agent:

```
neutron qos-bandwidth-limit-rule-create \
    --max-kbps 512 \
    --max-burst 1024 \
    example-policy
```
![arrow down](images/down_arrow.png)

```
tc qdisc add dev tap ingress handle ffff:
tc filter add dev tap parent ffff: protocol all prio 49 basic police \
    rate 512kbit burst 1024kbit mtu 65535 drop
```

???

So for example from slide about OVS such rule created on
LB agent will be applied with tc command with commands like shown on slide.
There are at least 3 commands to create limit because we need to create ingress
qdisc for interface and then apply filter rule on it to police rate and burst
values. Filter rule can match only some packets but we want (for now) to apply
it for all traffic incoming to tap interface (outgoing from VM)
For ingress qdisc TBF is used. On next slide I will show how tbf is working.

---

name: qos_how_tc_tbf_works
class: center

# How TBF qdisc works

![how tbf works](images/how_tbf_works.png)  
source: http://unix.stackexchange.com/a/100797

???
A "bucket" is a metaphorical object. Its key properties are that it can hold
tokens, and that the number of tokens it can hold is limited. If it overflows,
extra tokens are lost (just like trying to put too much water in an actual
bucket). __The size of the bucket is called burst__.

In order to actually transmit a packet onto the network, that packet must obtain
tokens equal to its size in bytes or mpu (whichever is larger).

There is (or can be) a line (queue) of packets waiting for tokens. This occurs
when the bucket is empty, or alternatively has fewer tokens than the size of the
packet. "Length" of this _sidewalk_ can be set indirectly with latency parameter
or directly in bytes.

When the kernel wants to send a packet out the filtered interface, it attempts
to place the packet at the end of the line. If there isn't any room on the
sidewalk, that's unfortunate for the packet, because at the end of the sidewalk
is a bottomless pit, and the kernel drops the packet.

---

name: qos_lb_config_options
class: center

# New config options for Linuxbridge agent:

* kernel_hz value

### How to calculate minimum burst value?
![how to calculate burst](images/how_to_calculate_burst.png)

* tbf_latency

???

Two new config options for LB agent added for QoS extension:
* kernel HZ value is required for calculate min burst value (this bucket size
  from previous slide) for tbf qdisc. If user will set bigger value then this min
  then user's value will be set otherwise this calculated value will be set to
  ensure proper working of TC limit.
* tbf_latency is required for tbf qdisc configuration. It's "sidewalk" lenght
  from one of previous slides
  (but it's more necessary for ingress traffic limit which will be implemented
  later)

---

# Side effect:

* L2 extension drivers in LinuxBridge agent
* Fullstack tests for LinuxBridge agent

    </textarea>
    <script src="remark-latest.min.js">
    </script>
    <script>
      var slideshow = remark.create();
    </script>
  </body>
</html>
